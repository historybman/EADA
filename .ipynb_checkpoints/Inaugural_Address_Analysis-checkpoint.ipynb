{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now for the Analysis\n",
    "After lots of munging, I have a dataframe containing text of all the presidential inaugural addresses, along with metadata like date, wordcounts, party affilations, and etc.\n",
    "\n",
    "First we open the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>urls</th>\n",
       "      <th>years</th>\n",
       "      <th>pres</th>\n",
       "      <th>texts</th>\n",
       "      <th>wordcounts</th>\n",
       "      <th>party</th>\n",
       "      <th>change</th>\n",
       "      <th>new counts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>http://avalon.law.yale.edu/18th_century/wash1.asp</td>\n",
       "      <td>1789</td>\n",
       "      <td>George Washington</td>\n",
       "      <td>of    THE CITY OF NEW YORK THURSDAY, APRIL 3...</td>\n",
       "      <td>1443.0</td>\n",
       "      <td>Federalist</td>\n",
       "      <td>New_Party</td>\n",
       "      <td>1443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>http://avalon.law.yale.edu/18th_century/wash2.asp</td>\n",
       "      <td>1793</td>\n",
       "      <td>George Washington</td>\n",
       "      <td>of    THE CITY OF PHILADELPHIA MONDAY, MARCH...</td>\n",
       "      <td>149.0</td>\n",
       "      <td>Federalist</td>\n",
       "      <td>Incumbent_Party</td>\n",
       "      <td>149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>http://avalon.law.yale.edu/18th_century/adams.asp</td>\n",
       "      <td>1797</td>\n",
       "      <td>John Adams</td>\n",
       "      <td>© 2008 Lillian Goldman Law Library 127 Wall St...</td>\n",
       "      <td>2417.0</td>\n",
       "      <td>Democratic-Republican</td>\n",
       "      <td>New_Party</td>\n",
       "      <td>2417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>http://avalon.law.yale.edu/19th_century/jefina...</td>\n",
       "      <td>1801</td>\n",
       "      <td>Thomas Jefferson</td>\n",
       "      <td>March 4, 1801 FRIENDS AND FELLOW-...</td>\n",
       "      <td>1742.0</td>\n",
       "      <td>Democratic-Republican</td>\n",
       "      <td>Incumbent_Party</td>\n",
       "      <td>1742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>http://avalon.law.yale.edu/19th_century/jefina...</td>\n",
       "      <td>1805</td>\n",
       "      <td>Thomas Jefferson</td>\n",
       "      <td>March 4, 1805  Proceeding, fellow citi...</td>\n",
       "      <td>2162.0</td>\n",
       "      <td>Democratic-Republican</td>\n",
       "      <td>Incumbent_Party</td>\n",
       "      <td>2162</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                urls  years  \\\n",
       "0  http://avalon.law.yale.edu/18th_century/wash1.asp   1789   \n",
       "1  http://avalon.law.yale.edu/18th_century/wash2.asp   1793   \n",
       "2  http://avalon.law.yale.edu/18th_century/adams.asp   1797   \n",
       "3  http://avalon.law.yale.edu/19th_century/jefina...   1801   \n",
       "4  http://avalon.law.yale.edu/19th_century/jefina...   1805   \n",
       "\n",
       "                pres                                              texts  \\\n",
       "0  George Washington    of    THE CITY OF NEW YORK THURSDAY, APRIL 3...   \n",
       "1  George Washington    of    THE CITY OF PHILADELPHIA MONDAY, MARCH...   \n",
       "2         John Adams  © 2008 Lillian Goldman Law Library 127 Wall St...   \n",
       "3   Thomas Jefferson               March 4, 1801 FRIENDS AND FELLOW-...   \n",
       "4   Thomas Jefferson          March 4, 1805  Proceeding, fellow citi...   \n",
       "\n",
       "   wordcounts                  party           change  new counts  \n",
       "0      1443.0             Federalist        New_Party        1443  \n",
       "1       149.0             Federalist  Incumbent_Party         149  \n",
       "2      2417.0  Democratic-Republican        New_Party        2417  \n",
       "3      1742.0  Democratic-Republican  Incumbent_Party        1742  \n",
       "4      2162.0  Democratic-Republican  Incumbent_Party        2162  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "inaugurals = pd.read_csv('./data/inaugurals_complete.csv')\n",
    "inaugurals.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os, csv, math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from collections import Counter\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.preprocessing import Normalizer\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now do some basic word-counting, using Ted's functions from Week 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the', 'of', 'and', 'to', 'in', 'a', 'our', 'that', 'we', 'be']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenize(astring):\n",
    "    ''' Breaks a string into words, and counts them.\n",
    "    Designed so it strips punctuation and lowercases everything,\n",
    "    but doesn't separate hashtags and at-signs.\n",
    "    '''\n",
    "    wordcounts = Counter()\n",
    "    # create a counter to hold the counts\n",
    "    \n",
    "    tokens = astring.split()\n",
    "    for t in tokens:\n",
    "        word = t.strip(',.!?:;-—()<>[]/\"\\'').lower()\n",
    "        wordcounts[word] += 1\n",
    "        \n",
    "    return wordcounts\n",
    "\n",
    "def addcounters(counter2add, countersum):\n",
    "    ''' Adds all the counts in counter2add to countersum.\n",
    "    Because Counters(like dictionaries) are mutable, it\n",
    "    doesn't need to return anything.\n",
    "    '''\n",
    "    \n",
    "    for key, value in counter2add.items():\n",
    "        countersum[key] += value\n",
    "\n",
    "def create_vocab(seq_of_strings, n):\n",
    "    ''' Given a sequence of text snippets, this function\n",
    "    returns the n most common words. We'll use this to\n",
    "    create a limited 'vocabulary'.\n",
    "    '''\n",
    "    vocab = Counter()\n",
    "    for astring in seq_of_strings:\n",
    "        counts = tokenize(astring)\n",
    "        addcounters(counts, vocab)\n",
    "    topn = [x[0] for x in vocab.most_common(n)]\n",
    "    return topn\n",
    "\n",
    "# Let's test the vocabulary function.\n",
    "vocab = create_vocab(inaugurals['texts'], 4000)\n",
    "vocab[0:10]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOP VALUES:\n",
      "two thousand 2000\n",
      "ten 10\n",
      "\n",
      "BOTTOM VALUES:\n",
      "neg one -1\n",
      "zero 0\n"
     ]
    }
   ],
   "source": [
    "def logodds(countsA, countsB, word):\n",
    "    ''' Straightforward.\n",
    "    '''\n",
    "    \n",
    "    odds = (countsA[word] + 1) / (countsB[word] + 1)\n",
    "    \n",
    "    # Why do we add 1 on both sides? Two reasons. The hacky one is \n",
    "    # that otherwise we'll get a division-by-zero error whenever\n",
    "    # word isn't present in countsB. The more principled reason\n",
    "    # is that this technique (called Laplacian smoothing) tends\n",
    "    # to reduce the dramatic disproportion likely to be found in\n",
    "    # very rare words.\n",
    "    \n",
    "    return math.log(odds)\n",
    "\n",
    "def signed_dunnings(countsA, totalA, countsB, totalB, word):\n",
    "    ''' Less straightforward. This function calculates a signed (+1 / -1)\n",
    "    version of Dunning's log likelihood. Intuitively, this is a number \n",
    "    that gets larger as the frequency of the word in our two corpora\n",
    "    diverges from its EXPECTED frequency -- i.e., the frequency it would\n",
    "    have if it were equally distributed over both. But it also tends to get\n",
    "    larger as the raw frequency of the word increases.\n",
    "    \n",
    "    Note that this function requires two additional arguments:\n",
    "    the total number of words in A and B. We could calculate that inside\n",
    "    the function, but it's faster to calculate it just once, outside the function.\n",
    "    \n",
    "    Also note: the strict definition of Dunnings has no 'sign': it gets bigger\n",
    "    whether a word is overrepresented in A or B. I've edited that so that Dunnings\n",
    "    is positive if overrepresented in A, and negative if overrepresented in B.\n",
    "    '''\n",
    "    if word not in countsA and word not in countsB:\n",
    "        return 0\n",
    "    \n",
    "    # the raw frequencies of this word in our two corpora\n",
    "    # still doing a little Laplacian smoothing here\n",
    "    a = countsA[word] + 0.1\n",
    "    b = countsB[word] + 0.1\n",
    "    \n",
    "    # now let's calculate the expected number of times this\n",
    "    # word would occur in both if the frequency were constant\n",
    "    # across both\n",
    "    overallfreq = (a + b) / (totalA + totalB)\n",
    "    expectedA = totalA * overallfreq\n",
    "    expectedB = totalB * overallfreq\n",
    "    \n",
    "    # and now the Dunning's formula\n",
    "    dunning = 2 * ((a * math.log(a / expectedA)) + (b * math.log(b / expectedB)))\n",
    "    \n",
    "    if a < expectedA:\n",
    "        return -dunning\n",
    "    else:   \n",
    "        return dunning\n",
    "\n",
    "# a set of common words is often useful\n",
    "stopwords = {'a', 'an', 'are', 'and', 'but', 'or', 'that', 'this', 'so', \n",
    "             'all', 'at', 'if', 'in', 'i', 'is', 'was', 'by', 'of', 'to', \n",
    "             'the', 'be', 'you', 'were'}\n",
    "\n",
    "# finally, one more function: given a list of tuples like\n",
    "testlist = [(10, 'ten'), (2000, 'two thousand'), (0, 'zero'), (-1, 'neg one'), (8, 'eight')]\n",
    "# we're going to want to sort them and print the top n and bottom n\n",
    "\n",
    "def headandtail(tuplelist, n):\n",
    "    tuplelist.sort(reverse = True)\n",
    "    print(\"TOP VALUES:\")\n",
    "    for i in range(n):\n",
    "        print(tuplelist[i][1], tuplelist[i][0])\n",
    "    \n",
    "    print()\n",
    "    print(\"BOTTOM VALUES:\")\n",
    "    lastindex = len(tuplelist) - 1\n",
    "    for i in range(lastindex, lastindex - n, -1):\n",
    "        print(tuplelist[i][1], tuplelist[i][0])\n",
    "        \n",
    "headandtail(testlist, 2)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOP VALUES:\n",
      "law 37.85900298929792\n",
      "business 32.87730435677244\n",
      "enforcement 27.661956717563825\n",
      "there 26.71258693822815\n",
      "no 21.023840498701418\n",
      "we 20.368952478920193\n",
      "freedom 19.949151468604846\n",
      "will 19.711108571025164\n",
      "make 19.27829134912094\n",
      "do 18.29887958039273\n",
      "negro 17.7213002579283\n",
      "amendment 17.662006465968325\n",
      "accept 17.01798088257794\n",
      "congress 16.84090924743422\n",
      "america 16.585253111580684\n",
      "method 14.88972168638792\n",
      "islands 14.88972168638792\n",
      "arbitration 14.88972168638792\n",
      "south 14.304357332097728\n",
      "prayer 13.7751544999991\n",
      "\n",
      "BOTTOM VALUES:\n",
      "which -25.587694865809624\n",
      "been -24.768251604369084\n",
      "powerful -24.257396715885317\n",
      "powers -20.639945618320855\n",
      "union -19.808186401517098\n",
      "myself -19.68520084932692\n",
      "me -19.212377774373138\n",
      "foreign -18.94148581831667\n",
      "my -18.87101149359202\n",
      " -18.74433136968643\n",
      "on -18.413088656786343\n",
      "opinion -16.987689023401476\n",
      "happy -16.890688937556916\n",
      "fellow-citizens -16.81270986608169\n",
      "spirit -16.686359964373743\n",
      "period -16.515374263165533\n",
      "limits -16.276798224583224\n",
      "measures -15.846495944172565\n",
      "country's -15.213400850163701\n",
      "democracy -14.591557939972915\n"
     ]
    }
   ],
   "source": [
    "# Code for Exercise 1\n",
    "\n",
    "# Start by creating a vocabulary for words in the Trump tweets.\n",
    "# Put it in a variable called 'vocab'.\n",
    "\n",
    "inaugural_text = inaugurals['texts']\n",
    "vocab = create_vocab(inaugural_text, 5000)\n",
    "vocab[:10]\n",
    "\n",
    "# Remember the function create_vocab takes two arguments:\n",
    "# (seq_of_strings, n)\n",
    "# We can afford to include all the words, so set n for 5000.\n",
    "\n",
    "\n",
    "# An optional step: removing stopwords\n",
    "vocab = list(set(vocab) - stopwords)\n",
    "\n",
    "# Create counters for the android and iphone corpora.\n",
    "\n",
    "Democrat = Counter()\n",
    "Republican = Counter()\n",
    "\n",
    "# Figure out how many rows are in the Trump DataFrame\n",
    "# and put that number in a variable like 'numrows.'\n",
    "# Then iterate through the 'text' column of the data frame.\n",
    "\n",
    "numrows = 58\n",
    "\n",
    "# for each text cell, get a Counter with words counts for that cell\n",
    "# then add those counts either to iphone or android, like so:\n",
    "\n",
    "for i in range(numrows):\n",
    "    counts = tokenize(inaugurals['texts'][i])\n",
    "    if 'Democrat' in inaugurals['party'][i]:\n",
    "        addcounters(counts, Democrat)\n",
    "    elif 'Republican' in inaugurals['party'][i]:\n",
    "        addcounters(counts, Republican)\n",
    "\n",
    "        \n",
    "# print(type(android))\n",
    "# When you get around to running Dunning's, you'll need to\n",
    "# create variables that hold the total count of *all words*\n",
    "# in iphone and android.\n",
    "total_democrat = sum(Democrat.values())\n",
    "total_republican = sum(Republican.values())\n",
    "\n",
    "# Create an empty list to hold pairs of (overrepresentation_measure, word)\n",
    "# Then iterate through your vocabulary. For each word, measure \n",
    "# overrepresentation using either logodds or signed_dunnings.\n",
    "# Create a tuple, (overrepresentation_measure, word)\n",
    "# and append it to the empty list you created.\n",
    "\n",
    "new_list = []\n",
    "for word in vocab:\n",
    "#     g = logodds(Republican, Democrat, word)\n",
    "    g = signed_dunnings(Republican, total_republican, Democrat, total_democrat, word)\n",
    "    new_list.append((g, word))\n",
    "    \n",
    "\n",
    "# Finally use the headandtail function to display the top 25 and bottom 25\n",
    "# words in your tuplelist.\n",
    "headandtail(new_list, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [py35]",
   "language": "python",
   "name": "Python [py35]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
